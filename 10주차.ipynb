{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8cbaXiY5322D/MNhEEHHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyuncastl/MACHINE_LEARNING/blob/main/10%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UJQCJ3sUqSC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naver sentiment movie corpus v1.0 데이터 불러오기\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    'ratings_train.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', extract=True)\n",
        "\n",
        "train = pd.read_csv(train_file, sep='\\t')"
      ],
      "metadata": {
        "id": "1mVyQJhVVVkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 크기 및 샘플 확인\n",
        "print(\"train shape: \", train.shape)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "WgGZHFNlVVh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train)"
      ],
      "metadata": {
        "id": "gfHFuFdXVVga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블별 개수\n",
        "cnt = train['label'].value_counts()\n",
        "print(cnt)"
      ],
      "metadata": {
        "id": "aLLsz8uYVVeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블별 비율 \n",
        "sns.countplot(x='label',data=train)"
      ],
      "metadata": {
        "id": "PWDDoH_WVVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "train.isnull().sum() "
      ],
      "metadata": {
        "id": "AjLkpJ4jVVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치(의견없음)가 특정 label값만 있는지 확인\n",
        "train[train['document'].isnull()]"
      ],
      "metadata": {
        "id": "CNJMkKM1VVYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 별 텍스트 길이\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "data_len=train[train['label']==1]['document'].str.len()\n",
        "ax1.hist(data_len)\n",
        "ax1.set_title('positive')\n",
        "\n",
        "data_len=train[train['label']==0]['document'].str.len()\n",
        "ax2.hist(data_len)\n",
        "ax2.set_title('negative')\n",
        "fig.suptitle('Number of characters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qo_BtFtQVVWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "metadata": {
        "id": "cWZv1PNxVVTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "metadata": {
        "id": "hFVOWVhHVVRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! bash install_mecab-ko_on_colab_light_220429.sh"
      ],
      "metadata": {
        "id": "oKUa7s2QVVPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mecab = Mecab()\n",
        "print(mecab.pos)"
      ],
      "metadata": {
        "id": "tkLEx6PzVVN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma, Komoran, Okt, Mecab 형태소\n",
        "import konlpy\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "DYuEibPKVVMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소별 샘플\n",
        "text = \"영실아안녕오늘날씨어때?\"\n",
        "\n",
        "def sample_ko_pos(text):\n",
        "    print(f\"==== {text} ====\")\n",
        "    print(\"kkma:\",kkma.pos(text))\n",
        "    print(\"komoran:\",komoran.pos(text))\n",
        "    print(\"okt:\",okt.pos(text))\n",
        "    print(\"mecab:\",mecab.pos(text))\n",
        "    print(\"\\n\")\n",
        "\n",
        "sample_ko_pos(text)"
      ],
      "metadata": {
        "id": "SenAqC5rVVKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리(영어와 한글만 남기고 삭제)\n",
        "train['document'] = train['document'].str.replace(\"[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "train['document'].head()"
      ],
      "metadata": {
        "id": "tS2zMzp3VVIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 제거\n",
        "train = train.dropna()\n",
        "train.shape"
      ],
      "metadata": {
        "id": "fH-dV1hwVVHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스탑워드와 형태소 분석\n",
        "def word_tokenization(text):\n",
        "  stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도'] # 한글 불용어\n",
        "  return [word for word in mecab.morphs(text) if word not in stop_words]"
      ],
      "metadata": {
        "id": "jLluLrGbVVFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = train['document'].apply((lambda x: word_tokenization(x)))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "RbiHHJRkVVDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 validation 분할\n",
        "\n",
        "training_size = 120000\n",
        "\n",
        "# train 분할\n",
        "train_sentences = data[:training_size]\n",
        "valid_sentences = data[training_size:]\n",
        "\n",
        "# label 분할\n",
        "train_labels = train['label'][:training_size]\n",
        "valid_labels = train['label'][training_size:]"
      ],
      "metadata": {
        "id": "NSZb_YXaVVBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# vocab_size 설정\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "print(\"총 단어 갯수 : \",len(tokenizer.word_index))\n",
        "\n",
        "# 5회 이상만 vocab_size에 포함\n",
        "def get_vocab_size(threshold):\n",
        "  cnt = 0\n",
        "  for x in tokenizer.word_counts.values():\n",
        "    if x >= threshold:\n",
        "      cnt = cnt + 1\n",
        "  return cnt\n",
        "\n",
        "vocab_size = get_vocab_size(5) # 5회 이상 출현 단어\n",
        "print(\"vocab_size: \", vocab_size)"
      ],
      "metadata": {
        "id": "L_1PjBS-VvKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_tok = \"\" # 사전에 없는 단어\n",
        "vocab_size = 15000\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(data)\n",
        "print(tokenizer.word_index)\n",
        "print(\"단어 사전 개수:\", len(tokenizer.word_counts))"
      ],
      "metadata": {
        "id": "acr8B-01VvIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자를 숫자로 표현\n",
        "print(train_sentences[:2])\n",
        "print(valid_sentences[:2])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
        "print(train_sequences[:2])\n",
        "print(valid_sentences[:2])"
      ],
      "metadata": {
        "id": "qYyFVXlHVvGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장의 최대 길이\n",
        "max_length = max(len(x) for x in train_sequences)\n",
        "print(\"문장 최대 길이:\", max_length)"
      ],
      "metadata": {
        "id": "S6GSEq_QVu8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 길이를 동일하게 맞춘다\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "valid_padded = pad_sequences(valid_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "train_labels = np.asarray(train_labels).reshape(-1,1)\n",
        "valid_labels = np.asarray(valid_labels).reshape(-1,1)\n",
        "\n",
        "print(\"샘플:\", train_padded[:1])"
      ],
      "metadata": {
        "id": "EtQrUQC_VU_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "                Embedding(vocab_size, 32),\n",
        "                Bidirectional(LSTM(32, return_sequences=True)),    \n",
        "                Dense(32, activation='relu'),\n",
        "                Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lsu_DGwUVU9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 좋은 loss의 가중치 저장\n",
        "checkpoint_path = 'best_performed_model.ckpt'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                save_weights_only=True, \n",
        "                                                save_best_only=True, \n",
        "                                                monitor='val_loss',\n",
        "                                                verbose=1)"
      ],
      "metadata": {
        "id": "Ox2_SWxdVU7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습조기종료\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "# 학습\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                validation_data=(valid_padded, valid_labels), \n",
        "                callbacks=[early_stop, checkpoint], batch_size=64, epochs=10, verbose=2)"
      ],
      "metadata": {
        "id": "A7Wz88tPVU5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "PPsR-in9V3_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "XEKGoajAV39j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 불러오기\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    'ratings_test.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt', extract=True)\n",
        "\n",
        "test = pd.read_csv(test_file, sep='\\t')\n",
        "test.head()\n",
        "     "
      ],
      "metadata": {
        "id": "h1XDcTK2V37w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def preprocessing(df):\n",
        "  df['document'] = df['document'].str.replace(\"[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "  df = df.dropna()\n",
        "  test_label = np.asarray(df['label'])\n",
        "  test_data =  df['document'].apply((lambda x: word_tokenization(x)))\n",
        "  test_data = tokenizer.texts_to_sequences(test_data)\n",
        "  test_data = pad_sequences(test_data, truncating=trunc_type, padding=padding_type, maxlen=max_length)\n",
        "  return test_data, test_label\n",
        "\n",
        "test_data, test_label = preprocessing(test)\n",
        "print(model.evaluate(test_data, test_label))"
      ],
      "metadata": {
        "id": "-mGdom8dV35q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 모델 로드 후 평가\n",
        "model2 = create_model()\n",
        "model2.evaluate(test_data, test_label)"
      ],
      "metadata": {
        "id": "tmL9aGPhV332"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 가중치 적용된 모델 로드 후 평가\n",
        "model2.load_weights(checkpoint_path)\n",
        "model2.evaluate(test_data, test_label)"
      ],
      "metadata": {
        "id": "0lcQMZgVV31E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "vjI7UFXhVU2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt() ##Open Korean Text 트위터에서 만든 형태소 분석기\n",
        "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔',stem = True)"
      ],
      "metadata": {
        "id": "HWKAbLupV_LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##문자열 치환 re.sub(검색패턴, 변경하고 싶은 문자, 검색되는 문자열) \n",
        "new_sentence = '이 영화 개꿀잼ㅎㅎㅎㅋㅋㅋ'\n",
        "new_sentence = re.sub(r'[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]','',new_sentence)\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "GFEQcbMtV_JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = okt.morphs(new_sentence, stem=True)\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "hyJODaArV_Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도']"
      ],
      "metadata": {
        "id": "cRdI2kOFWB_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = [word for word in new_sentence if not word in stop_words]\n",
        "new_sentence"
      ],
      "metadata": {
        "id": "YFogqpUZWB9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# vocab_size 설정\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "zGJandBxWB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_tok = \"\"#사전에 없는 단어\n",
        "vocab_size = 15000\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(new_sentence)\n",
        "     "
      ],
      "metadata": {
        "id": "YBLwqr8ZWHEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences(new_sentence)\n",
        "encoded"
      ],
      "metadata": {
        "id": "u-weHjHDWHBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_new=pad_sequences(encoded, maxlen=max_length)\n",
        "pad_new.shape"
      ],
      "metadata": {
        "id": "wtE3WqJPWG_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(pad_new).shape"
      ],
      "metadata": {
        "id": "A1mvzJgxWB57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(pad_new)"
      ],
      "metadata": {
        "id": "3ZyHE5AkWB35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=np.max(model2.predict(pad_new))\n",
        "print(score)"
      ],
      "metadata": {
        "id": "-cIZdoEkWKhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 74\n",
        "stop_words = [\"는\", \"을\", \"를\", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도']\n",
        "\n",
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = new_sentence.replace(\"[^A-Za-z-가-힣ㄱ-ㅎㅏ-ㅣ ]\",\"\")\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) #토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stop_words] #불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences(new_sentence) #정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) #패딩\n",
        "  score = np.max(model2.predict(pad_new)) #예측\n",
        "  if score > 0.5 :\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰 입니다.\\n\".format(score*100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰 입니다.\\n\".format(score*100))"
      ],
      "metadata": {
        "id": "lbDPAUYDWKfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ~~!!')"
      ],
      "metadata": {
        "id": "Yjr2Te3yWKd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
      ],
      "metadata": {
        "id": "j-RS1fJtWKcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
      ],
      "metadata": {
        "id": "fGnb7yZ3WKaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('감독 뭐하는 놈이냐?')"
      ],
      "metadata": {
        "id": "xFh3j7zIWRM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}